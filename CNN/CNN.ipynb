{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tensorflow)\n",
    "library(EBImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dataframe with files and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list.files('db', recursive = TRUE, full.names= TRUE)\n",
    "\n",
    "labels= lapply(strsplit(files, '[/]'), function(x){\n",
    "    x[[2]]\n",
    "    \n",
    "})\n",
    "labels = as.numeric(labels)\n",
    "\n",
    "data = data.frame('file' = files, 'label'= labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split images in train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = sample(x =  c(1:nrow(data)), size = round(0.8*nrow(data)) )\n",
    "train = data[split,]\n",
    "test = data[-split,]          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "aantal_pool  = 2\n",
    "aantal_kanalen = 64\n",
    "clas = as.integer(62)#number of classes\n",
    "schaal = 1 #scale of pixel values\n",
    "\n",
    "out = 0.5 #dropout\n",
    "batch_train = 50 #batchsize\n",
    "batch_test = 200 #batchsize test\n",
    "ds = 0.999 #gradient descent\n",
    "lr = 1e-4 #learningrate\n",
    "\n",
    "h = as.integer(60) #heigth image\n",
    "w = as.integer(60) #width image\n",
    "kanalen = as.integer(3) #chanals of image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- tf$placeholder(tf$float32, shape(NULL, h,w,kanalen), 'x')\n",
    "#target values\n",
    "labels <- tf$placeholder(tf$int64, shape(NULL), 'labels')\n",
    "#dropout rate\n",
    "keep_prob <- tf$placeholder(tf$float32, shape(),'keep_prob')\n",
    "#learningrate\n",
    "lrate <- tf$placeholder(tf$float32, shape(), 'lrate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_conv1 <-  tf$Variable( tf$truncated_normal(shape(5L, 5L, 3L, 40L),stddev=0.1), 'w_conv1')\n",
    "b_conv1 <- tf$Variable( tf$truncated_normal( shape(40L),stddev=0.1), 'b_conv1')\n",
    "\n",
    "w_conv2 <- tf$Variable( tf$truncated_normal(shape = shape(5L, 5L, 40L, 64L), stddev=0.1), 'w_conv2')\n",
    "b_conv2 <- tf$Variable( tf$truncated_normal(shape = shape(64L), stddev=0.1), 'b_conv2')\n",
    "\n",
    "w_conv3 <- tf$Variable( tf$truncated_normal(shape = shape(5L, 5L, 64L, 64L), stddev=0.1), 'w_conv3')\n",
    "b_conv3 <- tf$Variable( tf$truncated_normal(shape = shape(64L), stddev=0.1), 'b_conv3')\n",
    "\n",
    "w_fc1 <- tf$Variable( tf$truncated_normal(shape((w*h)/(4^(aantal_pool)) * aantal_kanalen, 1024L), stddev=0.1), 'w_fc1')\n",
    "b_fc1 <- tf$Variable( tf$truncated_normal(shape(1024L), stddev=0.1), 'b_fc1')\n",
    "\n",
    "w_output <- tf$Variable( tf$truncated_normal(shape(1024L, clas), stddev=0.1), 'w_output')\n",
    "b_output <- tf$Variable( tf$truncated_normal(shape(clas), stddev=0.1), 'b_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 <- tf$nn$relu( tf$nn$conv2d(x, w_conv1 , strides=c(1L, 1L, 1L, 1L), padding='SAME') + b_conv1)\n",
    "h_pool1 <- tf$nn$max_pool(h_conv1, ksize=c(1L, 2L, 2L, 1L),strides=c(1L, 2L, 2L, 1L), padding='SAME')\n",
    "h_conv2 <- tf$nn$relu( tf$nn$conv2d(h_pool1, w_conv2, strides=c(1L, 1L, 1L, 1L), padding='SAME') + b_conv2)\n",
    "h_pool2 <- tf$nn$max_pool(h_conv2,  ksize=c(1L, 2L, 2L, 1L),strides=c(1L, 2L, 2L, 1L), padding='SAME')\n",
    "h_conv3 <- tf$nn$relu( tf$nn$conv2d(h_pool2, w_conv3, strides=c(1L, 1L, 1L, 1L), padding='SAME') + b_conv3)\n",
    "h_conv3_flat <- tf$reshape(h_conv3, shape(-1L, (w*h)/(4^(aantal_pool)) * aantal_kanalen))\n",
    "h_fc1 <- tf$nn$relu(tf$matmul(h_conv3_flat, w_fc1) + b_fc1)\n",
    "h_fc1 <- tf$nn$relu(tf$matmul(h_conv3_flat, w_fc1) + b_fc1)\n",
    "h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)\n",
    "h_output <- tf$nn$softmax(tf$matmul(h_fc1_drop, w_output) + b_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf$nn$sigmoid_cross_entropy_with_logits( logits = h_output, labels = tf$one_hot(labels, clas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step <- tf$train$AdamOptimizer(lrate)$minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some variables to print along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction <- tf$equal(tf$argmax(h_output, 1L),labels)\n",
    "accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess <- tf$InteractiveSession()\n",
    "sess$run(tf$global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_images = function(files){\n",
    "    batch = array(0, dim = c(length(files), w, h, kanalen))\n",
    "for(i in 1:length(files)){\n",
    "    file = batch_files[i]\n",
    "    im = readImage( as.character(file))\n",
    "    im = resize(im, w=w, h=h)\n",
    "    \n",
    "    batch[i,,,] = im\n",
    "}\n",
    "    return(batch)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: 'NoneType' object is not callable\n\nDetailed traceback: \n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n    return fn(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n    status, run_metadata)\n\n",
     "output_type": "error",
     "traceback": [
      "Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: 'NoneType' object is not callable\n\nDetailed traceback: \n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n    return fn(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n    status, run_metadata)\n\nTraceback:\n",
      "1. sess$run(train_step, feed_dict = dict(x = batch_files, labels = batch_labels, \n .     keep_prob = out, lrate = ds^i * lr))",
      "2. py_call_impl(callable, dots$args, dots$keywords)"
     ]
    }
   ],
   "source": [
    "for (i in 1:20000) {\n",
    "  \n",
    "  #lees 50 random plaatjes in\n",
    " samp = sample( x=  c(1: nrow(train)) , size = batch_train )\n",
    "  \n",
    "  batch_labels = as.vector(train$label[samp])\n",
    "  batch_files = train$file[samp]\n",
    "\n",
    "batch_files = read_images(batch_files)\n",
    "  \n",
    "  #train met gradient descent\n",
    "  sess$run(train_step, feed_dict = dict(x = batch_files , labels = batch_labels , keep_prob = out, lrate = ds^i*lr))\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  #valideer om de 100 keer hoe het gaat op de testset\n",
    "  if (i %% 100 == 0) {\n",
    "    \n",
    "  \n",
    "    #evalueer op de testset\n",
    "    samp = sample( x=  c(1: nrow(test)) , size = batch_test )\n",
    "  \n",
    "  batch_labels = as.vector(test$label[samp])\n",
    "  batch_files = test$file[samp]\n",
    "\n",
    "batch_files = read_images(batch_files)\n",
    "  \n",
    "  #train met gradient descent\n",
    " test_accuracy =  sess$run(accuracy, feed_dict = dict(x = batch_files , labels = batch_labels , keep_prob = 1))\n",
    "  \n",
    "  \n",
    "    print( paste(\"step:\", i, \"test accuracy:\", test_accuracy) )    \n",
    "  }\n",
    "  \n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network sould be able to reach an accuracy of around 98 procent. If you do some data augmentation you can get over 99 percent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
